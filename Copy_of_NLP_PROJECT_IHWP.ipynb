{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the dataset**\n"
      ],
      "metadata": {
        "id": "6TtZ-bM1LTXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('/content/test.csv')\n",
        "\n",
        "# Convert the 'question' column to a list\n",
        "ques = df['question'].tolist()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gQXr0AEorwY6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env HUGGING_FACE_TOKEN=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"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkLZxckykSzy",
        "outputId": "6d537c11-8a84-4880-8b40-756ba847789c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: HUGGING_FACE_TOKEN=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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating and Paraphrasing Synthetic Questions Using Pre-trained Language Models**\n",
        "\n",
        "Here, using the GPT-2 for generating synthetic questions from a prompt and a T5-based model for paraphrasing questions to add variety. It integrates Hugging Face pipelines for both tasks, leveraging pre-trained models for text generation and rephrasing."
      ],
      "metadata": {
        "id": "4MF3glqnMP4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "import os\n",
        "\n",
        "# Load a large language model for synthetic data generation\n",
        "generator = pipeline('text-generation', model=\"gpt2\")\n",
        "\n",
        "# Function to generate synthetic questions based on a prompt\n",
        "set_seed(42)\n",
        "def generate_synthetic_questions(prompt, num_samples=5):\n",
        "    synthetic_questions = []\n",
        "    for _ in range(num_samples):\n",
        "        synthetic_question = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)[0][\"generated_text\"]\n",
        "        synthetic_questions.append(synthetic_question)\n",
        "    return synthetic_questions\n",
        "\n",
        "# Example of adding diversity by paraphrasing\n",
        "def paraphrase_question(question):\n",
        "    # If you have stored your Hugging Face token in an environment variable, retrieve it\n",
        "    token = os.getenv(\"HUGGING_FACE_TOKEN\")  # Ensure to set this environment variable with your token\n",
        "\n",
        "    # Initialize a paraphrasing pipeline with a T5-based model\n",
        "    paraphraser = pipeline(\"text2text-generation\", model=\"prithivida/parrot_paraphraser_on_T5\")\n",
        "\n",
        "    # Generate multiple paraphrases with beam search\n",
        "    paraphrased_questions = paraphraser(question, num_beams=3, num_return_sequences=3)\n",
        "    return [output[\"generated_text\"] for output in paraphrased_questions]\n"
      ],
      "metadata": {
        "id": "mOwW2sGckUrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e060d2-47d6-4ba2-afa9-ae7456cb3827"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvTOsTSpWsfY",
        "outputId": "4dfb74ae-a9bf-44df-ba30-c8f2e3d5c24c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiHRW9dq5Oum",
        "outputId": "db4ad649-1449-42b3-f5bd-a7bac049378a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Efficient Question Retrieval and Query Rewriting Using Sentence Transformers and FAISS**\n",
        "\n",
        "Using Sentence-Transformers to embed questions into dense vectors and indexes them in FAISS for efficient similarity-based retrieval. A query rewriting function is implemented using the Flan-T5 model, enhancing retrieval performance by refining the input query.\n"
      ],
      "metadata": {
        "id": "x3qaTEGTMgTb"
      }
    },
    {
      "source": [
        "!pip install transformers\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "\n",
        "# Load sentence transformer model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Build FAISS index\n",
        "def embed_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    embeddings = model(**inputs).last_hidden_state.mean(dim=1).detach().numpy()\n",
        "    return embeddings\n",
        "\n",
        "questions = ques  # Assuming 'ques' is defined elsewhere\n",
        "question_embeddings = np.vstack([embed_text(q) for q in questions])\n",
        "index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
        "index.add(question_embeddings)\n",
        "\n",
        "# Function to rewrite the query using a text generation model\n",
        "def rewrite_query(query):\n",
        "    query_rewriter = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")  # Initialize pipeline\n",
        "    rewritten_query = query_rewriter(query, max_length=50)  # Increase max_length\n",
        "    return rewritten_query\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5BBWAWZWDqcr",
        "outputId": "c3e54289-ee3b-4d05-e769-0cfa9384de2d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query Rewriting and Relevant Question Retrieval Using Sentence Transformers and FAISS**\n"
      ],
      "metadata": {
        "id": "O-1tgKzxM2YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"tell me about stoke laws\"\n",
        "# rewrite the original text query\n",
        "rewritten_query = rewrite_query(query)\n",
        "rewritten_query_text = rewritten_query[0]['generated_text']\n",
        "print(\"Rewritten query:\", rewritten_query[0]['generated_text']) # Print rewritten query\n",
        "distances, indices = index.search(embed_text(rewritten_query_text), k=10)  # Retrieve top 3 matches\n",
        "for i in indices[0]:\n",
        "    print(\"Relevant question:\", questions[i])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PwxGZnYFNOu",
        "outputId": "11794944-58eb-434c-f220-4ecb9fc11e78"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rewritten query: Stoke laws are laws that regulate the use of a stoke device.\n",
            "Relevant question: When we were dealing with electrical effects, it was very useful to speak of an electric field that surrounded what?\n",
            "Relevant question: One important phenomenon related to the relative strength of cohesive and adhesive forces is capillary action—the tendency of a fluid to be raised or suppressed in a narrow tube, or called this?\n",
            "Relevant question: No charge is actually created or destroyed when charges are separated as we have been discussing. rather, existing charges are moved about. in fact, in all situations the total amount of charge is always this?\n",
            "Relevant question: Newton’s second law of what is more than a definition; it is a relationship among acceleration, force, and mass?\n",
            "Relevant question: What term, calculated by multiplying heart contractions by stroke volume, means the volume of blood pumped by the heart in one minute?\n",
            "Relevant question: An object attached to a spring sliding on a frictionless surface is an uncomplicated type of what device?\n",
            "Relevant question: What is the term for the ability of a fluid to exert an upward force on any object placed in it?\n",
            "Relevant question: We must remember that the formal charge calculated for an atom is not the actual charge of the atom in the molecule. formal charge is only a useful bookkeeping procedure; it does not indicate the presence of these?\n",
            "Relevant question: In amphibians, reptiles, birds, and mammals, blood flow is directed in two circuits: one through the lungs and back to the heart, which is called what?\n",
            "Relevant question: In phyisics, what is considered to be the rotational version of force?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Semantic Search and Document Re-Ranking Using Bi-Encoder and Cross-Encoder Model**\n",
        "\n",
        "uses a Bi-Encoder model for efficient semantic search by embedding queries and documents into a vector space and retrieving top-k documents based on cosine similarity. The retrieved documents are further refined using a Cross-Encoder re-ranker, which scores query-document pairs for more accurate relevance ranking."
      ],
      "metadata": {
        "id": "Kne2bccL_l-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the pre-trained Bi-Encoder model for semantic search\n",
        "bi_encoder_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to(device)\n",
        "\n",
        "\n",
        "# Function to encode query and documents\n",
        "def encode_query_and_docs(query, documents, model):\n",
        "    # Encode the query and documents into vector space\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True, device=device)\n",
        "    doc_embeddings = model.encode(documents, convert_to_tensor=True, device=device)\n",
        "    return query_embedding, doc_embeddings\n",
        "\n",
        "# Function to retrieve top-k relevant documents\n",
        "def retrieve_top_k_documents(query, documents, model, top_k=3):\n",
        "    # Encode query and documents\n",
        "    query_embedding, doc_embeddings = encode_query_and_docs(query, documents, model)\n",
        "\n",
        "    # Compute cosine similarity between query and documents\n",
        "    similarities = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]\n",
        "\n",
        "    # Get top-k most relevant documents\n",
        "    top_k_indices = torch.topk(similarities, k=top_k).indices\n",
        "    top_k_documents = [(documents[idx], similarities[idx].item()) for idx in top_k_indices]\n",
        "    return top_k_documents\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "reranker_model_name = \"cross-encoder/nli-deberta-v3-base\"\n",
        "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name).to(device)\n",
        "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
        "\n",
        "# Function to re-rank documents using a re-ranker\n",
        "def rerank_documents(query, top_k_documents, model, tokenizer):\n",
        "    reranked_docs = []\n",
        "    for doc, score in top_k_documents:\n",
        "        # Prepare the input for cross-encoder (query-document pair)\n",
        "        inputs = tokenizer(query, doc, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "        outputs = model(**inputs)\n",
        "        # Extract the maximum value across the logits\n",
        "        rerank_score = outputs.logits[0].max().item()  # Get the max score across the logits\n",
        "        reranked_docs.append((doc, rerank_score))\n",
        "\n",
        "    # Sort by re-ranker score in descending order\n",
        "    reranked_docs = sorted(reranked_docs, key=lambda x: x[1], reverse=True)\n",
        "    return reranked_docs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "feGWVtBa4k-G",
        "outputId": "893d538f-88c4-4cc3-8b52-5b1569796783"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top-k Document Retrieval Using Bi-Encoder Semantic Search**\n",
        "\n",
        "Retrieving the top-k relevant documents for a given query by encoding both the query and a document set using a Bi-Encoder model. It ranks the documents based on cosine similarity, providing a list of the most relevant matches with their scores."
      ],
      "metadata": {
        "id": "iRS3ikZnNNVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"what is newton law of motion\"\n",
        "top_k_documents = retrieve_top_k_documents(query, ques, bi_encoder_model, top_k=10)\n",
        "print(\"Top-k Relevant Documents:\")\n",
        "for i, (doc, score) in enumerate(top_k_documents, 1):\n",
        "    print(f\"{i}. {doc} (Score: {score:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoRrsUnFPgMq",
        "outputId": "1169bbf1-83b1-4715-c562-8af5ae3ef483"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-k Relevant Documents:\n",
            "1. Newton’s second law of what is more than a definition; it is a relationship among acceleration, force, and mass? (Score: 0.6592)\n",
            "2. What is the study of how forces affect the motion of objects? (Score: 0.5277)\n",
            "3. What is the term for the combined forces acting on an object? (Score: 0.4624)\n",
            "4. In physics, when one subtracts the frictional force from the applied force what is the result? (Score: 0.4305)\n",
            "5. In phyisics, what is considered to be the rotational version of force? (Score: 0.4261)\n",
            "6. What is the force that opposes motion between two surfaces that are touching? (Score: 0.4229)\n",
            "7. What is the term for the force that brings objects toward the earth? (Score: 0.4204)\n",
            "8. What force occurs because no surface is perfectly smooth? (Score: 0.4052)\n",
            "9. What is a vector quantity with the same direction as the force called? (Score: 0.3710)\n",
            "10. When explosion of gases creates pressure resulting in motion of a rocket, the force pushing the rocket is called what? (Score: 0.3682)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Re-ranking Retrieved Documents Using Cross-Encoder**\n",
        "\n",
        "Refining the initial top-k retrieved documents by using a Cross-Encoder model to score query-document pairs. This re-ranking process ensures more accurate prioritization of documents based on relevance, outputting the final sorted list with updated scores.\n"
      ],
      "metadata": {
        "id": "R8AmWN28NgfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-rank the documents using the reranker model\n",
        "reranked_documents = rerank_documents(query, top_k_documents, reranker_model, reranker_tokenizer)\n",
        "\n",
        "# Print the re-ranked documents with formatted scores\n",
        "print(\"\\nRe-ranked Documents:\")\n",
        "for i, (doc, score) in enumerate(reranked_documents, start=1):\n",
        "    # Check if the score is a tensor and inspect its shape\n",
        "    if isinstance(score, torch.Tensor):\n",
        "        print(f\"Score tensor shape: {score.shape}\")\n",
        "        # If the tensor has more than one element, take the mean or max\n",
        "        if score.numel() == 1:\n",
        "            score = score.item()  # Single element, directly convert to scalar\n",
        "        else:\n",
        "            score = score.max().item()  # Take the max score if it's a vector of logits\n",
        "    print(f\"{i}. {doc} (Re-rank Score: {score:.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1ffuW5kPnBQ",
        "outputId": "2bf2bcff-a51f-4cd0-d142-02073dc25728"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Re-ranked Documents:\n",
            "1. What force occurs because no surface is perfectly smooth? (Re-rank Score: 4.5590)\n",
            "2. When explosion of gases creates pressure resulting in motion of a rocket, the force pushing the rocket is called what? (Re-rank Score: 4.2261)\n",
            "3. In physics, when one subtracts the frictional force from the applied force what is the result? (Re-rank Score: 3.8636)\n",
            "4. What is the force that opposes motion between two surfaces that are touching? (Re-rank Score: 2.8803)\n",
            "5. In phyisics, what is considered to be the rotational version of force? (Re-rank Score: 1.8775)\n",
            "6. What is the term for the combined forces acting on an object? (Re-rank Score: 1.8697)\n",
            "7. Newton’s second law of what is more than a definition; it is a relationship among acceleration, force, and mass? (Re-rank Score: 1.7521)\n",
            "8. What is a vector quantity with the same direction as the force called? (Re-rank Score: 1.1486)\n",
            "9. What is the term for the force that brings objects toward the earth? (Re-rank Score: 1.1434)\n",
            "10. What is the study of how forces affect the motion of objects? (Re-rank Score: 1.1071)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n"
      ],
      "metadata": {
        "id": "Qr6uJYyk_GCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2KICwoZB90uy",
        "outputId": "838047ad-a37c-4d25-a3db-36d63d58214a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (3.0.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Device Configuration:\n",
        "\n",
        "Determines whether to use GPU or CPU for computations.\n",
        "Bi-Encoder Embedding:\n",
        "Utilizes the msmarco-distilbert-base-v4 model to generate vector embeddings for the query and documents for semantic search.\n",
        "\n",
        "\n",
        "LLM Judge:\n",
        "\n",
        "* Loads a GPT-2 model for binary relevance classification between query and retrieved results.\n",
        "Metric 1 - LLM-Based Precision:\n",
        "\n",
        "* Evaluates precision by scoring each result as relevant (1) or non-relevant (0) based on the GPT-2 output logits.\n",
        "Metric 2 - SacreBLEU:\n",
        "\n",
        "* Computes BLEU scores for query-document pairs to assess linguistic similarity.\n",
        "\n",
        "Metric 3 - Cosine Precision at k:\n",
        "\n",
        "* Uses cosine similarity between query and document embeddings to determine whether the most relevant document appears in the top-k results.\n",
        "Evaluation:\n",
        "\n",
        "Calculates and displays scores for LLM-based precision, SacreBLEU, and cosine precision metrics.\n"
      ],
      "metadata": {
        "id": "rZR9QSfLNyyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import sacrebleu\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load Bi-Encoder for vector embeddings\n",
        "# bi_encoder_model = SentenceTransformer('paraphrase-MiniLM-L6-v2').to(device)\n",
        "\n",
        "# Load LLM judge (Mistral model)\n",
        "llm_judge_model_name = \"gpt2\"\n",
        "llm_judge_model = AutoModelForSequenceClassification.from_pretrained(llm_judge_model_name).to(device)\n",
        "llm_judge_tokenizer = AutoTokenizer.from_pretrained(llm_judge_model_name)\n",
        "\n",
        "# Sample queries and model outputs\n",
        "# query = \"Can you provide examples of questions about plant hormones?\"\n",
        "top_k_results = top_k_documents\n",
        "\n",
        "# Metric 1: LLM-based Precision\n",
        "def llm_based_precision(query, top_k_results, model, tokenizer):\n",
        "    precision_scores = []\n",
        "    for result in top_k_results:\n",
        "        # Prepare query and result for LLM evaluation\n",
        "        input_text = f\"Query: {query}\\nQuestion: {result}\"\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "        outputs = model(**inputs)\n",
        "        # Assume the logits indicate relevance (1 or 0)\n",
        "        relevance_score = torch.argmax(outputs.logits).item()  # Binary: 1 for relevant, 0 otherwise\n",
        "        precision_scores.append(relevance_score)\n",
        "\n",
        "    # Calculate average precision\n",
        "    precision = sum(precision_scores) / len(precision_scores)\n",
        "    return precision\n",
        "\n",
        "# Metric 2: SacreBLEU\n",
        "def sacrebleu_score(query, top_k_results):\n",
        "    # SacreBLEU expects a reference and a list of hypothesis\n",
        "    bleu_scores = [sacrebleu.sentence_bleu(doc, [query]).score for doc, _ in top_k_results]\n",
        "    return sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "# Metric 3: Cosine Precision at k = 5\n",
        "def cosine_precision_at_k(query, top_k_results, model, k=5):\n",
        "    # Encode query and results\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True, device=device)\n",
        "    # Extract only the document strings from top_k_results\n",
        "    result_docs = [doc for doc, _ in top_k_results]\n",
        "    result_embeddings = model.encode(result_docs, convert_to_tensor=True, device=device)\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    similarities = util.pytorch_cos_sim(query_embedding, result_embeddings)[0]\n",
        "\n",
        "    # Sort results by similarity\n",
        "    sorted_indices = torch.argsort(similarities, descending=True)\n",
        "    top_k_indices = sorted_indices[:k]\n",
        "\n",
        "    # Assume target document is the first result (synthetic example)\n",
        "    target_document = top_k_results[0]  # Assuming ground truth at index 0\n",
        "    target_embedding = model.encode(target_document, convert_to_tensor=True, device=device)\n",
        "\n",
        "    # Calculate precision: Check if target document is in top-k\n",
        "    target_in_top_k = any(torch.equal(result_embeddings[idx], target_embedding) for idx in top_k_indices)\n",
        "    precision = 1.0 if target_in_top_k else 0.0\n",
        "    return precision\n",
        "\n",
        "# Evaluation\n",
        "llm_precision = llm_based_precision(query, top_k_results, llm_judge_model, llm_judge_tokenizer)\n",
        "bleu_score = sacrebleu_score(query, top_k_results)\n",
        "cosine_precision = cosine_precision_at_k(query, top_k_results, bi_encoder_model)\n",
        "\n",
        "# Display Results\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(f\"LLM-based Precision: {llm_precision:.2f}\")\n",
        "print(f\"SacreBLEU Score: {bleu_score:.2f}\")\n",
        "print(f\"Cosine Precision at k=5: {cosine_precision:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBKTdHDp6KM7",
        "outputId": "2118fdec-0536-4c7a-ba6c-778bbb3c5036"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics:\n",
            "LLM-based Precision: 0.00\n",
            "SacreBLEU Score: 3.81\n",
            "Cosine Precision at k=5: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing the Discounted Cumulative Gain (DCG) for a list of relevances and normalizes it to calculate the normalized DCG (nDCG). nDCG is used to evaluate the ranking quality of a set of results by comparing it to the ideal ranking."
      ],
      "metadata": {
        "id": "k20NGgWzOzZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def dcg(relevances):\n",
        "    return np.sum([(2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(relevances)])\n",
        "\n",
        "def ndcg(relevances, ideal_relevances):\n",
        "    return dcg(relevances) / dcg(ideal_relevances)\n",
        "\n",
        "# Sample usage\n",
        "relevances = [3, 2, 3, 0, 1]\n",
        "ideal_relevances = sorted(relevances, reverse=True)\n",
        "print(\"nDCG:\", ndcg(relevances, ideal_relevances))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUi9Ot0rgVl9",
        "outputId": "111414f8-1cd9-4584-bc8a-83dab253a07b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nDCG: 0.9574784666412695\n"
          ]
        }
      ]
    }
  ]
}